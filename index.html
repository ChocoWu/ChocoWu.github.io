<!-- <!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN"> -->
<!DOCTYPE HTML>
<html>

<head>
  <meta name=viewport content="width=800">
  <meta name="generator" content="HTML Tidy for Linux/x86 (vers 11 February 2007), see www.w3.org">
  <meta charset="utf-8">
  <style type="text/css">
    /* Color scheme stolen from Sergey Karayev */
    /*Codes for this website are borrowed from https://zexuehe.github.io/*/
    
    a {
      color: #1772d0;
      text-decoration: none;
    }
    
    a:focus,
    a:hover {
      color: #f09228;
      text-decoration: none;
    }
    
    body,
    td,
    th,
    tr,
    p,
    a {
      font-family: 'Lato', Verdana, Helvetica, sans-serif;
      font-size: 16px;
      line-height:25px
    }
    td{
      padding-left: initial;
    }
    strong {
      font-family: 'Lato', Verdana, Helvetica, sans-serif;
      font-size: 16px;
    }
    
    heading {
      font-family: 'Lato', Verdana, Helvetica, sans-serif;
      font-size: 30px;
    }
    
    papertitle {
      font-family: 'Lato', Verdana, Helvetica, sans-serif;
      font-size: 14px;
      font-weight: 700
    }
    
    name {
      font-family: 'Lato', Verdana, Helvetica, sans-serif;
      font-size: 32px;
    }
    
    .one {
      width: 160px;
      height: 160px;
      position: relative;
    }
    
    .two {
      width: 160px;
      height: 160px;
      position: absolute;
      transition: opacity .2s ease-in-out;
      -moz-transition: opacity .2s ease-in-out;
      -webkit-transition: opacity .2s ease-in-out;
    }
    
    .fade {
      transition: opacity .2s ease-in-out;
      -moz-transition: opacity .2s ease-in-out;
      -webkit-transition: opacity .2s ease-in-out;
    }
    
    span.highlight {
      background-color: #ffffd0;
    }


    li::marker {
    font-size: 1.2em;
    }

    ul li{
      height: 30;
    }
  </style>
  <link rel="icon" type="image/png" href="images/wsq.png">
  <title>Shengqiong Wu</title>
  <meta http-equiv="Content-Type" content="text/html; charset=us-ascii">
  <link href='https://fonts.googleapis.com/css?family=Lato:400,700,400italic,700italic' rel='stylesheet' type='text/css'>
</head>

<body>
  <table width="950" border="0" align="center" cellspacing="0" cellpadding="0">
    <tr>
      <td>

        <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
          <tr>
            <td width="70%" valign="middle">
              <p align="center">
                <name>Shengqiong Wu</name>
              </p>
              <p>I am currently a second-year Ph.D. student at <a href="https://www.nextcenter.org/">NExT++</a> Research Center, advised by Prof. <a href="https://www.chuatatseng.com/">Tat-Seng Chua</a> in <a href="https://www.comp.nus.edu.sg">School of Computing</a> at <a href="https://www.nus.edu.sg">National University of Singapore</a>, after I obtained my M.S. and Bachelor degrees from <a href="https://www.whu.edu.cn/">Wuhan University</a>.
              </p>
              <p>
                My research interest mainly lies in <strong>Scene Graph-based Vision-Language Understanding</strong>, with the contexts of <strong>Multimodal Large Language Models</strong> and <strong>Diffusion Model</strong>, etc.
                Previously, I focused on syntax-aided NLP, i.e., employing external syntactic and linguistic representation to aid NLP tasks.
              </p> 
              
              <p align=center>
                <a href="mailto:swu@u.nus.edu">Email</a> &nbsp/&nbsp
                <a href="https://github.com/ChocoWu">Github</a> &nbsp/&nbsp
                <a href="https://scholar.google.com/citations?user=RJJLKR0AAAAJ">Google Scholar</a>
<!--                &nbsp/&nbsp-->
<!--                 <a href="data/CV_wsq.pdf">CV</a> &nbsp -->
                
              </p>
            </td>
            <td width="30%">
              <img src="images/ME.jpg" width="80%" style="box-shadow: 0px 10px 20px rgba(0,0,0,0.19), 0px 6px 6px rgba(0,0,0,0.23);">
            </td>
          </tr>
        </table>

      <!-- NEWs -->
      <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20" style="background-color: rgba(246, 249, 253, 0.878);">
        <tr>
          <td width="100%" valign="middle">
            <heading>üî•Newsüî•</heading><br>
            <li> <span style="color: red;">[NEW!-2024/08]</span> üêæI‚Äôm excited to have the opportunity to be a <em>volunteer</em> at ACL 2024. Looking forward to being part of it!</li>
            <li> <span style="color: red;">[NEW!-2024/08]</span> I have tried to build <a href="https://aistudio.baidu.com/projectdetail/8168280">NExT-GPT</a> in <a href="https://www.paddlepaddle.org.cn/">PaddlePaddle</a>, <a href="https://github.com/PaddlePaddle/PaddleNLP">PaddleNLP</a> and <a href="https://github.com/PaddlePaddle/PaddleMIX/tree/release/2.0/ppdiffusers">PPDiffusers</a>. Still working on it. ü§îü§î</li>
            <li> <span style="color: red;">[NEW!-2024/05]</span> Congratulations! ü•≥ü•≥ü•≥, two full papers are accpted in ICML-24, <a href="">NExT-GPT</a>  and  <a href="">Video-of-Thought</a> .</li>
            <li> <span style="color: red;">[NEW!-2024/04]</span> We release Vitron (<a href="https://vitron-llm.github.io/">Demo</a>, <a href="http://haofei.vip/downloads/papers/Skywork_Vitron_2024.pdf">Paper</a> , <a href="https://github.com/SkyworkAI/Vitron">Code</a>), a universal pixel-level vision LLM designed for understanding, generating, segmenting, editing of both image and video. üåüüåü</li>
            <li> <span style="color: red;">[NEW!-2024/02]</span> One full paper is accepted by CVPR-24 about <a href="https://haofei.vip/Dysen-VDM/">Text-to-Video Generation</a>. Congrats to all my co-authors. </li>
            <li> <span style="color: red;">[NEW!-2024/01]</span> I honor Baidu Scholarship (10 people worldwide). ü•≥ü•≥ü•≥ </li>
            <li> <span style="color: red;">[NEW!-2023/12]</span> I have successfully passed my Rearcsh-based QE, I am now a Ph.D. Candidate. ü•≥ü•≥ü•≥  </li>
            <li> <span style="color: red;">[NEW!-2023/11]</span> I'll join <a href="">Kunlun 2050 Research</a>  as a research intern, advised by <a href="https://yanshuicheng.info/">Prof. Yan</a>. üòÜ </li>
            <li> <span style="color: red;">[NEW!-2023/10]</span> We build <a href="https://next-gpt.github.io/">NExT-GPT</a>, a general-purpose any-to-any MLLM. üåü </li>
            <li> <span style="color: red;">[NEW!-2023/09]</span> One full paper is accepted in <a href="https://neurips.cc/Conferences/2023">NeurIPS-23</a>, about <a href="https://openreview.net/pdf?id=hSTaTBIUCj">Intricate Text-to-image Generation</a>  based on Scene Graph. ü•≥</li>
            <li> <span style="color: red;">[NEW!-2023/07]</span> One full paper is accepted in <a href="https://www.acmmm2023.org/">ACM MM-23</a>, about <a href="https://arxiv.org/pdf/2308.05095.pdf">High-faithfull Text-to-image Generation</a> enhanced with layout.</li>
            <li> <span style="color: red;">[NEW!-2023/05]</span> Two full papers is accepted in <a href='https://2023.aclweb.org/'>ACL-23</a>, about <a href='https://aclanthology.org/2023.acl-long.823.pdf'>Multimodal Relation Extraction</a> and <a href='https://aclanthology.org/2023.acl-long.146.pdf'>Image Captioning.</a></li>
            <li> <span style="color: red;">[NEW!-2022/07]</span> I'm heading to <a href="https://www.comp.nus.edu.sg/">SoC NUS</a> to pursue my PhD‚Äîa new journey and fresh challenges await! üôÉ</a></li>
          </td>
        </tr>
      </table>

      <hr style="width:100%; border: 0.7px dashed rgba(228, 228, 228, 0.878);">
      
        <!-- research -->
        <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
          <tr>
            <td width="100%" valign="middle">
              <heading>Publications</heading>
              
            </td>
          </tr>
        </table>

        <!-- cellpadding="20" -->
        <table width="100%" align="center" border="0" cellspacing="0" cellpadding="0">
            <tbody>
              <tr>
                <td width="100%"><strong>2024</strong></td>
              </tr>
              <tr>
                <td width="100%">
                  <ul>
                    <li>
                      <strong>Shengqiong Wu</strong>, Hao Fei, Xiangtai Li, Jiayi Ji, Hanwang Zhang, Tat-Seng Chua, Shuicheng Yan.&nbsp&nbsp<em>Towards Semantic Equivalence of Tokenization in Multimodal LLM</em>. <strong>arXiv</strong>. 2024.&nbsp&nbsp<a href="https://chocowu.github.io/SeTok-web/">[Code]</a><a href="https://arxiv.org/pdf/2406.05127">[pdf]</a>
                    </li>
                    <li>
                      Meng Luo, Hao Fei*, Bobo Li, <strong>Shengqiong Wu</strong>, Qian Liu, Soujanya Poria, Erik Cambria, Mong-Li Lee, Wynne Hsu.&nbsp&nbsp<em>PanoSent: A Panoptic Sextuple Extraction Benchmark for Multimodal Conversational Aspect-based Sentiment Analysis</em>. In Proceedings of <strong>ACM MM</strong>. 2024.&nbsp&nbsp (Oral).
                    </li>
                    <li>
                      <strong>Shengqiong Wu</strong>, Hao Fei, Leigang Qu, Wei Ji, Tat-Seng Chua.&nbsp&nbsp<em>NExT-GPT: Any-to-Any Multimodal Large Language Model</em>. In Proceedings of <strong>ICML</strong>. 2024.&nbsp&nbsp (Oral) <a href="https://next-gpt.github.io/">[Code | 3.2k üåü]</a><a href="https://arxiv.org/pdf/2309.05519.pdf">[pdf]</a>
                    </li>
                    <li>
                      Hao Fei, <strong>Shengqiong Wu</strong>, Wei Ji, Hanwang Zhang, Meishan Zhang, Mong-Li Lee, Wynne Hsu.&nbsp&nbsp<em>Video-of-Thought: Step-by-Step Video Reasoning from Perception to Cognition</em>. In Proceedings of <strong>ICML</strong>. 2024.&nbsp&nbsp (Oral) <a href="https://github.com/scofield7419/Video-of-Thought">[Code]</a><a href="https://haofei.vip/downloads/papers/VoT_2024.pdf">[pdf]</a> 
                    </li>
                    
                    <li>
                      Hao Fei, <strong>Shengqiong Wu</strong>, Wei Ji, Hanwang Zhang, Tat-Seng Chua.&nbsp&nbsp<em>Dysen-VDM: Empowering Dynamics-aware Text-to-Video Diffusion with LLMs</em>. In Proceedings of <strong>CVPR</strong>. 2024.&nbsp&nbsp<a href="https://haofei.vip/Dysen-VDM/">[Code]</a><a href="https://arxiv.org/pdf/2308.13812">[pdf]</a>
                    </li>
                    <li>
                      Hao Fei, <strong>Shengqiong Wu</strong>, Hanwang Zhang, Tat-Seng Chua, Shuicheng Yan.&nbsp&nbsp<em>VITRON: A Unified Pixel-level Vision LLM for Understanding, Generating, Segmenting, Editing</em>. <strong>arXiv</strong>. 2024.&nbsp&nbsp<a href="https://vitron-llm.github.io/">[Code]</a><a href="http://haofei.vip/downloads/papers/Skywork_Vitron_2024.pdf">[pdf]</a>
                    </li>
                    </ul>
                </td>
              </tr>

              <tr>
                <td width="100%"><strong>2023</strong></td>
              </tr>
              <tr>
                <td width="100%">
                <ul>
                  
                  <li>
                    <strong>Shengqiong Wu</strong>, Hao Fei, Hanwang Zhang, Tat-Seng Chua.&nbsp&nbsp<em>Imagine That! Abstract-to-Intricate Text-to-Image Synthesis with Scene Graph Hallucination Diffusion</em>. In Proceedings of <strong>NeurIPS</strong>. 2023.&nbsp&nbsp(long, poster)&nbsp&nbsp<a href="https://github.com/ChocoWu/T2I-Salad">[Code]</a><a href="https://openreview.net/forum?id=hSTaTBIUCj">[pdf]</a>
                  </li>
                  <li>
                    Leigang Qu*, <strong>Shengqiong Wu*</strong>, Hao Fei, Liqiang Nie, Tat-Seng Chua.&nbsp&nbsp<em>LayoutLLM-T2I: Eliciting Layout Guidance from LLM for Text-to-Image Generation</em>. In Proceedings of <strong>ACM MM</strong>. 2023.&nbsp&nbsp(*: equal contribution, long)&nbsp&nbsp<a href="https://layoutllm-t2i.github.io/">[Code]</a><a href="https://arxiv.org/pdf/2308.05095.pdf">[pdf]</a>
                  </li>
                  <li>
                    Bobo Li, Hao Fei, Yuhan Wu, Jinsong Zhang, <strong>Shengqiong Wu</strong>, Jingye Li, Yijiang Liu, Lizi Liao, Tat-Seng Chua, Fei Li, Donghong Ji.&nbsp&nbsp<em>DiaASQ: A benchmark of conversational aspect-based sentiment quadruple analysis</em>.In Proceedings of <strong>ACL</strong>. 2023.&nbsp&nbsp(long, poster)&nbsp&nbsp<a href="https://github.com/unikcc/DiaASQ">[Code]</a><a href="https://aclanthology.org/2023.findings-acl.849.pdf">[pdf]</a>
                  </li>
                  <li>
                    <strong>Shengqiong Wu</strong>, Hao Fei, Yixin Cao, Lidong Bing, Tat-Seng Chua.&nbsp&nbsp<em>Information Screening whilst Exploiting! Multimodal Relation Extraction with Feature Denoising and Multimodal Topic Modeling</em>. In Proceedings of <strong>ACL</strong>. 2023.&nbsp&nbsp(long, poster, paper award nomination, 1.6%)&nbsp&nbsp<a href="https://github.com/ChocoWu/MRE-ISE">[Code]</a><a href="https://aclanthology.org/2023.acl-long.823.pdf">[pdf]</a>
                  </li>
                  <li>
                   <strong>Shengqiong Wu</strong>, Hao Fei, Wei Ji, Tat-Seng Chua.&nbsp&nbsp<em>Cross2StrA: Unpaired Cross-lingual Image Captioning with Cross-lingual Cross-modal Structure-pivoted Alignment</em>. In Proceedings of <strong>ACL</strong>. 2023.&nbsp&nbsp(long, oral)&nbsp&nbsp<a href="https://aclanthology.org/2023.acl-long.146.pdf">[pdf]</a>
                  </li>
                </ul>
                </td>
              </tr>

              <tr>
                <td width="100%"><strong>2022</strong></td>
              </tr>
              <tr>
                <td width="100%">
                <ul>
                  <li>
                    Hao Fei, <strong>Shengqiong Wu</strong>, Jingye Li, Bobo Li, Fei Li, Libo Qin, Meishan Zhang, Min Zhang, Tat-Seng Chua.&nbsp&nbsp<em>LasUIE: Unifying information extraction with latent adaptive structure-aware generative language model</em>. In Proceedings of <strong>NeurIPS</strong>. 2022.&nbsp&nbsp(long, poster) <a href="https://github.com/ChocoWu/LasUIE">[Code]</a><a href="https://proceedings.neurips.cc/paper_files/paper/2022/file/63943ee9fe347f3d95892cf87d9a42e6-Paper-Conference.pdf">[pdf]</a>
                  </li>
                  <li>
                    Shunjie Chen, Xiaochuan Shi, Jingye Li, <strong>Shengqiong Wu</strong>, Hao Fei, Fei Li, Donghong Ji.&nbsp&nbsp<em>Joint Alignment of Multi-Task Feature and Label Spaces for Emotion Cause Pair Extraction</em>. In Proceedings of <strong>COLING</strong>. 2022.&nbsp&nbsp(long, poster) <a href="https://github.com/csj199813/A2Net_ECPE">[Code]</a><a href="https://arxiv.org/abs/2209.04112">[pdf]</a>
                  </li>
                  <li>
                    Hu Cao, Jingye Li, Fangfang Su, Fei Li, Hao Fei, <strong>Shengqiong Wu</strong>, Bobo Li, Liang Zhao and Donghong Ji.&nbsp&nbsp<em>OneEE: A One-Stage Framework for Fast Overlapping and Nested Event Extraction</em>. In Proceedings of <strong>COLING</strong>. 2022.&nbsp&nbsp(long, oral) <a href="https://github.com/Cao-Hu/OneEE">[Code]</a><a href="https://arxiv.org/abs/2209.02693">[pdf]</a>
                  </li>
                  <li>
                    <strong>Shengqiong Wu</strong>, Hao Fei, Fei Li, Meishan Zhang, Yijiang Liu, Chong Teng, Donghong Ji.&nbsp&nbsp<em>Mastering the Explicit Opinion-Role Interaction: Syntax-Aided Neural Transition System for Unified Opinion Role Labeling</em>. In Proceedings of <strong>AAAI</strong>. 2022.&nbsp&nbsp(long, online) <a href="https://github.com/ChocoWu/SyPtrTrans-ORL">[Code]</a><a href="https://ojs.aaai.org/index.php/AAAI/article/view/21404">[pdf]</a>
                  </li>
                  <li>
                    Jingye Li, Hao Fei, Jiang Liu, <strong>Shengqiong Wu</strong>, Meishan Zhang, Chong Teng, Donghong Ji, Fei Li.&nbsp&nbsp<em>Unified named entity recognition as word-word relation classification</em>. In Proceedings of <strong>AAAI</strong>. 2022.&nbsp&nbsp(long, online) <a href="https://github.com/ljynlp/W2NER">[Code]</a><a href="https://ojs.aaai.org/index.php/AAAI/article/view/21344">[pdf]</a>
                  </li>
                </ul>
                </td>
              </tr>


              <tr>
                <td width="100%"><strong>2021</strong></td>
              </tr>
              <tr>
                <td width="100%"><ul>
                  <li>
                    <strong>Shengqiong Wu</strong></u>, Hao Fei,  Yafeng Ren, Donghong Ji, Jingye Li.&nbsp&nbsp<em>Learn from Syntax: Improving Pair-wise Aspect and Opinion Terms Extraction with Rich Syntactic Knowledge</em>. In Proceedings of <strong>IJCAI</strong>. 2021.&nbsp&nbsp(long, online) <a href="https://github.com/ChocoWu/Synfue-PAOTE">[Code]</a><a href="https://arxiv.org/pdf/2105.02520.pdf">[pdf]</a>
                  </li>
                  <li>
                      <strong>Shengqiong Wu</strong>, Hao Fei, Yafeng Ren, Bobo Li, Fei Li and Donghong Ji.&nbsp&nbsp<em> High-order Pair-wise Aspectand Opinion Terms Extraction with Edge-enhanced Syntactic Graph Convolution</em>.  IEEE/ACM <strong>TASLP</strong>. 2021. <a href="https://github.com/ChocoWu/ESGCN-AOP">[Code]</a><a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=9478183">[pdf]</a>
                  </li>
                  <li>
                    <strong>Shengqiong Wu</strong>, Bobo Li, Dongdong Xie, Chong Teng and Donghong Ji.&nbsp&nbsp<em>Neural Transition Model for Aspect-based Sentiment Triplet Extraction with Triplet Memory</em>. <strong>Neurocomputing</strong>. 2021.&nbsp&nbsp <a href="https://www.sciencedirect.com/science/article/pii/S0925231221011887">[pdf]</a>
                  </li>
                </ul>
                </td>
              </tr>


              <tr>
                <td width="100%"><strong>2020</strong></td>
              </tr>
              <tr>
                <td width="100%"><ul>
                  <li>
                    <strong>ShengqiongWu</strong>, Hao Fei and Donghong Ji.&nbsp&nbsp<em>Aggressive Language Detection with Joint Text Normalization via Adversarial Multi-task Learning</em>. In Proceedings of <b>NLPCC</b>. 2020.&nbsp&nbsp(long, oral) <a href="https://github.com/ChocoWu/ALD-TN">[Code]</a><a href="https://link.springer.com/chapter/10.1007%2F978-3-030-60450-9_54">[pdf]</a>
                  </li>
                </ul></td>
              </tr>


              


            </tbody>
          </table>

        <hr style="width:100%; border: 0.7px dashed rgba(228, 228, 228, 0.878);">


        <!-- Academic Services -->
          <table width="100%" align="center" border="0" cellspacing="0" cellpadding="0" style="background-color: rgba(246, 249, 253, 0.878);">
            <tr>
              <td width="100%" valign="middle">
                <heading>Academic Services</heading><br><br>
                <tr>
                  <td width="100%"><strong>Conference Reviewer</strong></td>
                </tr>
                <tr>
                  <td style="padding-left: 40px;" width="100%">
                      WSDM-23, NeurIPS-23, ICLR-24, ICML-24, ACM MM-23/24, IJCAI-23/24, AAAI-24, ACL-23/24
                    </td>
                </tr>
                <tr>
                  <td style="padding-top: 4px;" width="100%"><strong>Journal Reviewer</strong></td>
                </tr>
                <tr>
                  <td style="padding-left: 40px;" width="100%">
                    <!-- <li> -->
                      TOMM, IEEE/ACM TALLIP, IPM, KBS, Neurocomputing
                    <!-- </li> -->
                  </td>
                </tr>
                
                <!-- <li> <b>[Conference Reviewer]</b> &nbsp&nbsp&nbsp&nbsp&nbsp WSDM-23, NeurIPS-23, ICLR-24, ICML-24, ACM MM-23/24, IJCAI-23/24, AAAI-24</li>
                <li> <b>[Journal Reviewer]</b> &nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp TOMM, IEEE/ACM TALLIP, IPM, KBS, Neurocomputing</li> -->
              </td>
            </tr>
          </table>
    

          <hr style="width:100%; border: 0.7px dashed rgba(228, 228, 228, 0.878);">

          <!-- Invited Talks -->
          <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
            <tr>
              <td width="100%" valign="middle">
                <heading>Invited Talks</heading>
                <ol style="list-style-type: none;  padding-left: 0; margin-left: 0; margin-top: 20px;">
                  <li> <b>[2024/01]</b> &nbsp&nbsp&nbsp&nbsp&nbsp <a href="https://event.baai.ac.cn/activities/744">The Path to AGI: Achieving Modality Unification with NExT-GPT</a>. Qingyuan Talk.</li>
                  <li> <b>[2023/12]</b> &nbsp&nbsp&nbsp&nbsp&nbsp <a href="https://zhidx.com/p/406693.html">NExT-GPT: Any-to-Any Multimodal LLM</a>.  AI New Youth.</li>
                  <li> <b>[2022/12]</b> &nbsp&nbsp&nbsp&nbsp&nbsp Deep Learning based Natural Language Processing: A Survey and Outlook.  Jianghan University, China.</li>
                  <li> <b>[2021/10]</b> &nbsp&nbsp&nbsp&nbsp&nbsp Comparison of Aspect-based Sentiment Analysis based on span model and transition model.  Wuhan University, China.</li>
                  <li> <b>[2021/05]</b> &nbsp&nbsp&nbsp&nbsp&nbsp Learn from Syntax: Improving Pair-wise Aspect and Opinion Terms Extraction with Rich Syntactic Knowledge.  CIPS, Youth Working Committee.</li>
                </ol>
              </td>
            </tr>
          </table>


        <hr style="width:100%; border: 0.7px dashed rgba(228, 228, 228, 0.878);">


        <!-- Work Experience -->
        <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20" style="background-color: rgba(246, 249, 253, 0.878);">
          <tbody><tr>
            <td width="100%" valign="middle">
            <heading>Work Experience</heading>
        
            <table>
              <tbody>
                <tr>
                  <td width="90%">
                      <br>
                      Nov. 2023 - Jun. 2024 &nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp <b>Kunlun Skywork AI</b></a>, Singapore<br>
                        &nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp 2050 Research Intern <br>
                        <!-- &nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp <i>Crawl web-data related to financial stock. Construct knowledge graph for Smart Q&A. Employ graph database (Neo4j) for data storage.</i><br> -->
                        &nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp Advisor: <a href="https://yanshuicheng.info/"> Shuicheng Yan, Director </a> <br>
                  </td>
      
                  <td width="30%">
                        <img src="images/kunlun.png" width="100%" style="display: block; margin-left: auto; margin-right: auto;">
                  </td>
                </tr>
                <tr>
                    <td width="90%">
                        <br>
                        Jul. 2019 - Aug. 2019 &nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp <b>China Merchants Bank</b></a>, Wuhan, China<br>
                          &nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp Information Technology Department Intern <br>
                          <!-- &nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp <i>Crawl web-data related to financial stock. Construct knowledge graph for Smart Q&A. Employ graph database (Neo4j) for data storage.</i><br> -->
                          &nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp Advisor: <a href="https://www.linkedin.com/in/%E5%8D%8E-%E6%BD%98-83949a110/?originalSubdomain=cn"> Hua Pan, General Manager </a> <br>
                    </td>
        
                    <td width="30%">
                          <img src="images/CMB.png" width="100%" style="display: block; margin-left: auto; margin-right: auto;">
                    </td>
                </tr>
                <tr>
                    <td width="90%">
                        <br>
                        Jul. 2018 - Nov. 2018 &nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp <b>YITU</b></a>, ShangHai, China<br>
                          &nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp Solution Engineer Intern <br>
                          <!-- <i>Responsible for the deployment, upgrade, daily maintenance of products and equipment in the company's projects.</i><br> -->
                          &nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp Advisor: <a herf='https://www.linkedin.com/in/%E6%B3%BD-%E9%82%93-4045b0157/'>Ze Deng</a><br>
                    </td>
        
                    <td width="30%">
                          <img src="images/yitu.png" width="100%" style="display: block; margin-left: auto; margin-right: auto;">
                    </td>
                </tr>
              </tbody>
          </table>
        </td>
      </tr>
      </tbody></table>


      <hr style="width:100%; border: 0.7px dashed rgba(228, 228, 228, 0.878);">



        <!-- honors and awards -->  
        <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
          <tbody><tr>
            <td width="100%" valign="middle">
              <heading>Honors &amp; Awards</heading><br><br>
              <li>[2024] &nbsp&nbsp <a href="https://www.sohu.com/a/754077156_132450">Scholarship (Top 10 Global Recipients)</a>, Baidu.</li>
              <li>[2023] &nbsp&nbsp Research Achievement Award of SoC, NUS.</li>
              <li>[2022] &nbsp&nbsp President's Graduate Fellowship of NUS.</li>
              <li>[2022] &nbsp&nbsp Award of Excellent Master Thesis of Hubei Cyberspace Security Society.</li>
              <li>[2022] &nbsp&nbsp Excellent Graduated Graduate Student.</li>
              <li>[2021] &nbsp&nbsp Huawei Scholarship for graduate students.</li>
              <li>[2021] &nbsp&nbsp National Scholarship for graduate students.</li>
              <li>[2021] &nbsp&nbsp 1st prize of Excellent Scholarship of Academy for M.S. Fellowship.</li>
              <li>[2021] &nbsp&nbsp 1st prize of Extraordinary scholarship of academy.</li>
              <li>[2020] &nbsp&nbsp Outstanding graduate student award.</li>
              <li>[2020] &nbsp&nbsp 2st prize of academic scholarship of Wuhan University for graduate.</li>
              <li>[2017] &nbsp&nbsp Excellent student cadre.</li>
              <li>[2016-2019] &nbsp&nbsp 2st prize of academic scholarship of Wuhan University for undergraduate.</li>
              <li>[2017] &nbsp&nbsp National Encouragement Scholarship.</li>
            </td>
          </tr>
          </tbody></table>
        
          <hr style="width:100%; border: 0.7px dashed rgba(228, 228, 228, 0.878);">


        <!-- Education -->
        <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20" style="background-color: rgba(246, 249, 253, 0.878);">
          <tbody><tr>
            <td width="100%" valign="middle">
              <heading>Education</heading>
              <table>
                <tbody>
                  <tr>
                    <td width="90%">
                        <br>
                        <strong>Aug. 2022 - Now</strong> &nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp  School of Computing, &nbsp <strong>National University of Singapore</strong>, &nbsp Singapore<br>
                        &nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp &nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp Ph.D. in Computer Science <br> 
                        <!-- &nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp &nbsp &nbsp&nbsp &nbsp &nbsp &nbsp&nbsp&nbsp Sep. 2019 - Jun. 2022 expected. -->
                    </td>
                    <td width="10%">
                      <img src="images/NUS.png" width="90%" style="display: block; margin-left: auto; margin-right: auto;">
                    </td>
                  </tr>
                  <tr>
                    <td width="90%">
                        <br>
                        <strong>Sep. 2019 - Jun. 2022</strong> &nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp  School of Cyber Science and Engineering, &nbsp <strong>Wuhan University</strong>, &nbsp China<br>
                        &nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp &nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp M.S. in Computer Science <br> 
                    </td>
                    <td width="10%">
                      <img src="images/WHU.png" width="90%" style="display: block; margin-left: auto; margin-right: auto;">
                    </td>
                  </tr>
                  <tr>
                  <td width="90%">
                      <br>
                      <strong> Sep. 2015 - Jun. 2019 </strong> &nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp School of Computer Science, &nbsp <strong>Wuhan University</strong>, &nbsp China<br>
                      &nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp &nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp B.S. in Computer Science and Technology
                  </td>
                  <td width="10%">
                    <img src="images/WHU.png" width="90%" style="display: block; margin-left: auto; margin-right: auto;">
                  </td>
                </tr>
              </tbody></table>
            </td>
          </tr>
          </tbody></table>
      
          
          <hr style="width:100%; border: 0.7px dashed rgba(228, 228, 228, 0.878);">



        <!-- Skills and MISC -->
        <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
          <tbody><tr>
            <td width="100%" valign="middle">
              <heading>Skills and MISC</heading><br><br>
              <li>
                <b>Coding</b><br>
                <img src="images/python.png">&nbsp&nbsp&nbsp&nbsp <img src="images/java.png">&nbsp&nbsp&nbsp&nbsp <img src="images/latex.png">&nbsp&nbsp&nbsp&nbsp <img src="images/markdown.png">&nbsp&nbsp&nbsp&nbsp <img src="images/html.png">&nbsp&nbsp&nbsp&nbsp <img src="images/css.png">&nbsp&nbsp&nbsp&nbsp <img src="images/js.png">&nbsp&nbsp&nbsp&nbsp <img src="images/ubuntu.png">...
              </li>
              <li>
                <b>Deep Learning</b><br>
                <img src="images/pytorch.png">&nbsp&nbsp&nbsp&nbsp <img src="images/tensorflow.png">...
              </li>
              <li>
                <b>Hobby</b><br>
                <img src="images/badminton.png">&nbsp&nbsp&nbsp&nbsp <img src="images/table_tennis.png">...
              </li>
      
            </td>
          </tr>
          </tbody></table>



       <script type="text/javascript">
          var gaJsHost = (("https:" == document.location.protocol) ? "https://ssl." : "http://www.");
          document.write(unescape("%3Cscript src='" + gaJsHost + "google-analytics.com/ga.js' type='text/javascript'%3E%3C/script%3E"));
        </script>
        <script type="text/javascript">
          try {
            var pageTracker = _gat._getTracker("UA-7580334-1");
            pageTracker._trackPageview();
          } catch (err) {}
        </script>
        </td>
    </tr>
  </table>
</body>

</html>
